<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>用Tensorflow实现简单的单变量线性回归</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">用Tensorflow实现简单的单变量线性回归</span></h1>

<h2 class="date">2018/08/06</h2>
</div>

<main>
<p>这里用的是 tf 底层 API，先导入所需要的包，并设置好一些超参数</p>

<pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

LEARNING_RATE = 0.001
EPOCHES = 200
BATCH_SIZE = 10
</code></pre>

<p>然后自己生成随机数据点，用以稍后的训练，顺便把数据点先画好</p>

<pre><code class="language-python">HOW_MANY = 80  # 数据点个数

true_w = int(np.random.randn() * 7)
true_b = int(np.random.randn() * 4)
# 真正的 w 与 b，为了方便比对，均取整

train_X = np.random.rand(HOW_MANY) * 15
noise = np.random.randn(HOW_MANY) * 3.5  # 随机噪音，有正有负
train_Y = true_w * train_X + true_b + noise

plt.plot(train_X, train_Y, 'o')
plt.plot(train_X, train_Y - noise, 'g', label='real')

print('generate successfully')
</code></pre>

<p>设置所需节点</p>

<pre><code class="language-python">X = tf.placeholder(tf.float32)
Y = tf.placeholder(tf.float32)

w = tf.Variable(tf.zeros(()))
b = tf.Variable(tf.zeros(()))  # w and b are all scalars
</code></pre>

<p>设置模型（线性）和 loss function、optimizer</p>

<pre><code class="language-python">pred = w * X + b
loss = tf.reduce_sum((Y - pred) ** 2)

optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)
</code></pre>

<p>初始化 Variables</p>

<pre><code class="language-python">with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
</code></pre>

<p>分批投喂数据进行训练</p>

<pre><code class="language-python"># 仍在 with 里边
    for i in range(EPOCHES):
        for batch in range(0, HOW_MANY, BATCH_SIZE):
            sess.run(optimizer, feed_dict={X: train_X[batch:batch+BATCH_SIZE], Y: train_Y[batch:batch+BATCH_SIZE]})
        if (i+1) % 5 == 0:
            print('Epoch #{}, loss = {}'.format(i+1, sess.run(loss, feed_dict={X: train_X, Y: train_Y})))
</code></pre>

<p>训练完成，输出相关信息，完成图表并展示</p>

<pre><code class="language-python"># 仍在 with 里边
    print('Trained successfully.')
    print('true_w = {}, true_b = {}'.format(true_w, true_b))
    print('pred_w = {}, pred_b = {}'.format(w.eval(), b.eval()))

    _x = np.linspace(0, 15)
    _y = w.eval() * _x + b.eval()

plt.plot(_x, _y.T, 'r', label='prediction')
plt.show()
</code></pre>

<p><del>如果学习率设置得太高，可能会导致在 Optimizer 找到合适方向前，loss function 的值就已经爆表了，</del>最后得到的 w 跟 b 都是 NaN。所以比较稳妥的方法是，减小学习率，增加训练次数。</p>

<blockquote>
<p>Update: 对于随机梯度下降来说，<a href="https://www.zhihu.com/question/66200879/answer/239468705">Tensorflow 会自动记录节点的运算规则并求导</a>，不存在“找不到合适方向”的情况。不过如果 <code>loss function</code> 越来越高的话，基本都是由于学习率过高，不断跨过沟底。同时，沟壁的梯度（一般）越远离极小值点则越大（所以更新的步伐也越来越大），最终造成 <code>loss function</code> 的值越来越离谱。</p>
</blockquote>

<p>试着跑了一下，能得到如下输出：</p>

<pre><code class="language-python">true_w = -3, true_b = 0
pred_w = -2.9783382415771484, pred_b = 0.7763855457305908
</code></pre>

<p><img src="univariate-linear-regression.png" alt="运行效果" /></p>

</main>


<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
  
  
  if (window.location.hostname == "localhost")
    return;

  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  var disqus_shortname = 'oyiadin';
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  <footer>
  <script src="/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>



  
  <hr/>
  &copy; <a href="https://blog.b1n.top/">oyiadin</a> | <a href="https://github.com/oyiadin">Github</a> | <a href="https://twitter.com/oyiadin">Twitter</a>
  
  </footer>
  </body>
</html>

