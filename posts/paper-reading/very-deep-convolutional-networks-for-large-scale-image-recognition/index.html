<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>\[Paper-Reading\] very deep convolutional networks for large scale image recognition (ICLR 2015)</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/about/">About</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
      <li><a href="/index.xml">Subscribe</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">[Paper-Reading] very deep convolutional networks for large scale image recognition (ICLR 2015)</span></h1>

<h2 class="date">2019/01/25</h2>
</div>

<main>


<h2 id="abstract">Abstract</h2>

<p>这篇论文探究了在大规模的图像识别场景中，CNN 网络的深度对其准确率的影响。论文中，作者使用了一个足以捕获空域信息的最小 conv filter (3x3) 来减少参数数量，并逐渐增加网络的深度。于是有了这样的发现：当深度加深时，模型的准确度有着显著的提高，甚至超过了当时的最佳水平。并且，这种小卷积核、大深度的网络有着很好的泛化能力，在其他领域也有着很不错的表现。</p>

<h2 id="sec1-introduction">Sec1: Introduction</h2>

<p>已经出现过两种尝试提升准确度的方向：</p>

<ol>
<li>改进原始模型 <a href="#" title="Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net- works. In NIPS, pp. 1106–1114, 2012.">Krizhevsky et al. (2012)</a> 的架构</li>
<li>training and testing the networks densely over the whole image and over multiple scales <a href="#" title="
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.">Sermanet et al., 2014</a>; <a href="#" title="Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc.
ICLR, 2014.&quot;">Howard, 2014</a></li>
</ol>

<p>这篇论文里重点探究网络深度的影响（方法：固定其他参数(如 FC 层数)，通过增加 conv layers 的方式逐渐增加网络深度）</p>

<p>作者提供了几个<a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/">表现最好的模型</a>，不过是 caffe 用的。作者在<a href="http://www.vlfeat.org/matconvnet/pretrained/">这里</a>还放出了许多 matlab 的版本，可以用 <code>scipy.io.loadmat(path)</code> 的方法在 Python 里导入。</p>

<h2 id="sec2-convnet-configurations">Sec2: ConvNet Configurations</h2>

<h3 id="2-1-architecture">2.1 Architecture</h3>

<p>训练过程中，输入均是 224x224 的 RGB 格式图片，唯一的预处理是减去 RGB 的均值。</p>

<p>设计这个架构的主要思路是，先让图片通过一堆卷积层，然后再通过三层 FC 层，最后那层是一个 1000 channels 的 softmax。激活函数一致选用 ReLU。</p>

<p>在卷积层中，将会使用很小的感知域/卷积核（3x3 是足以感知上下、左右、中心点等概念的最小尺寸）。在其中一个实验中，作者还尝试了 1x1 的卷积核，这(单指这次卷积操作)可以视为对输入的线性变换（注意后边的 max-pooling 依然是非线性的）。所有 conv 层的 padding 设置都基于不让空间的解析度发生变化这一目的（保持 224x224）。部分 conv 层后会接上 max-pooling，performed over a 2x2 pixel windows with stride 2.</p>

<h3 id="2-2-configurations">2.2 Configurations</h3>

<p>见下表，每个模型按列展示，conv 层命名格式为 <code>conv⟨receptive field size⟩-⟨number of channels⟩</code>。其中，LRN 是 <code>Local Response Normalisation</code> 的意思，作者试了一下感觉没用，所以后边就不加这个了。</p>

<p><img src="model-configurations.png" alt="model configurations" /></p>

<h3 id="2-3-discussion">2.3 Discussion</h3>

<p>作者还论证了一下自己的模型比 ILSVRC-2012, 2013 中最好的模型还牛批。不仅参数减少了（归功于卷积核 3x3 的大小），准确度也提升了。</p>

<p>3 层的 3x3 conv layers 就相当于有了 7x7 的感知域，但是引入了三层非线性的变换，这使得 3x3 与 7x7 的模型相比，增加了模型的判别能力（discriminative，不知道怎么翻译合适），还减少了参数的数量 (from <code>$49C^2$</code> to <code>$27C^2$</code>)</p>

<p>小卷积核的思路在其他人的论文里已经出现过了，但是他们的模型深度没有 vgg 的深，也没有在大规模数据集上进行评估。或者是拓扑结构比 vgg 的复杂。总结：vgg 牛批！=。=</p>

<h2 id="sec3-classification-framework">Sec3: Classification Framework</h2>

<p>训练方法：带动量的 mini-batch 梯度下降</p>

<ul>
<li>batch_size = 256</li>
<li>momentum = 0.9</li>
<li>regularization_penalty = <code>$5*10^{-4}$</code> (L2)</li>
<li>dropout_ratio = 0.5</li>
<li>learning_rate = <code>$10^{-2}$</code> (decresed by a factor of 10 gradually)</li>
</ul>

<p>作者说网络的权重初始化很重要，一不小心就可能造成梯度的不稳定。为了避免这个问题，作者从 A 网络(见上边的表)开始训练，这个网络足够浅，随机地进行初始化就行了。在训练其他更深的网络时，前四层 conv layers 跟最后的三层 FC layers 会以训练完毕的网络 A 中对应的参数值进行初始化，其他中间层则随机进行即可。这些“预初始化”的模型保持原始的 learning_rate 参数，不会因为已经进行过“预初始化”而减小初始学习率。</p>

<p>进行随机初始化时，<code>$X \sim N(0, 10^{-2})$</code> while <code>$b = 0$</code>。</p>

<p>为了得到固定大小 (224x224) 的图片，训练集中的图片被放缩后进行随机的切割。为了扩充训练集，这些“切片”还经过了随机的水平翻转与 RGB 颜色变换。</p>

<blockquote>
<p>论文中剩下的部分我没怎么看懂……先跳过</p>
</blockquote>

<p>最后论文说在 ILSVRC 里，他们用的是 7 个模型的融合(fusion)，没有细看，方法是 averaging their soft-max class posteriors。</p>

<p>之后回炉重新看的时候再补充。</p>

</main>


<div id="disqus_thread"></div>
<script type="text/javascript">

(function() {
  
  
  if (window.location.hostname == "localhost")
    return;

  var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
  var disqus_shortname = 'oyiadin';
  dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
  (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  <footer>
  <script src="/js/math-code.js"></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>



  
  <hr/>
  &copy; <a href="https://blog.b1n.top/">oyiadin</a> | <a href="https://github.com/oyiadin">Github</a> | <a href="https://twitter.com/oyiadin">Twitter</a>
  
  </footer>
  </body>
</html>

